# LiteLLM Proxy Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: litellm
  labels:
    app.kubernetes.io/name: litellm
    app.kubernetes.io/part-of: mlops-platform
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # https://docs.litellm.ai/docs/proxy/configs
    
    model_list:
      # Primary model: Mistral-7B via vLLM
      - model_name: "mistral-7b"
        litellm_params:
          model: "openai/mistral-7b-instruct"
          api_base: "http://mistral-7b-instruct-predictor.serving.svc.cluster.local:8000/v1"
          api_key: "not-needed"
          max_tokens: 8192
          temperature: 0.7
        model_info:
          id: "mistral-7b"
          description: "Mistral 7B Instruct model served via vLLM"
          max_tokens: 8192
          input_cost_per_token: 0.0
          output_cost_per_token: 0.0
      
      # Secondary model: Llama-2-7B via vLLM
      - model_name: "llama-7b"
        litellm_params:
          model: "openai/llama-2-7b-chat"
          api_base: "http://llama-7b-chat-predictor.serving.svc.cluster.local:8000/v1"
          api_key: "not-needed"
          max_tokens: 4096
          temperature: 0.7
        model_info:
          id: "llama-7b"
          description: "Llama 2 7B Chat model served via vLLM"
          max_tokens: 4096
          input_cost_per_token: 0.0
          output_cost_per_token: 0.0
      
      # Embedding model via Ray Serve (optional)
      # - model_name: "text-embedding"
      #   litellm_params:
      #     model: "openai/text-embedding-3-small"
      #     api_base: "http://embedding-service.ray-system.svc.cluster.local:8000"
      #     api_key: "not-needed"
    
    router_settings:
      routing_strategy: "least-busy"
      num_retries: 3
      timeout: 120
      retry_after: 5
      cooldown_time: 60
      allowed_fails: 3
      
      # Model fallbacks
      fallbacks:
        - mistral-7b: ["llama-7b"]
        - llama-7b: ["mistral-7b"]
    
    general_settings:
      # Disable for dev; enable with external secret in production
      master_key: "sk-litellm-dev-key"
      
      # Rate limiting
      global_max_parallel_requests: 100
      max_request_size_mb: 10
      
      # Caching with Redis
      cache: true
      cache_params:
        type: "redis"
        host: "redis-master.redis.svc.cluster.local"
        port: 6379
        ttl: 3600
      
      # Logging
      success_callback: ["prometheus"]
      failure_callback: ["prometheus"]
    
    litellm_settings:
      # Request timeout
      request_timeout: 120
      
      # Connection pool
      max_parallel_requests: 50
      
      # Telemetry
      telemetry: false
      drop_params: true
